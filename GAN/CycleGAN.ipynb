{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝 kears-crontrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /private/var/folders/46/b7dzk4mn6g54qzptv608w7d00000gn/T/pip-t1_7py2z-build\n",
      "Requirement already satisfied: keras in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras-contrib==2.0.8)\n",
      "Requirement already satisfied: pyyaml in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Installing collected packages: keras-contrib\n",
      "  Running setup.py install for keras-contrib ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed keras-contrib-2.0.8\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "        #print(path)\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "        #print(path_A)\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取馬對斑馬轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:480: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:483: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[-0.89019608, -0.84313725, -0.86666667],\n",
       "         [-0.8745098 , -0.81176471, -0.85882353],\n",
       "         [-0.92156863, -0.89019608, -0.89019608],\n",
       "         ...,\n",
       "         [-0.48235294, -0.21568627, -0.65490196],\n",
       "         [-0.55294118, -0.31764706, -0.70196078],\n",
       "         [-0.79607843, -0.57647059, -0.84313725]],\n",
       "\n",
       "        [[-0.89019608, -0.81960784, -0.85882353],\n",
       "         [-0.80392157, -0.71764706, -0.77254902],\n",
       "         [-0.82745098, -0.74901961, -0.80392157],\n",
       "         ...,\n",
       "         [-0.76470588, -0.59215686, -0.75686275],\n",
       "         [-0.71764706, -0.60784314, -0.7254902 ],\n",
       "         [-0.81960784, -0.56862745, -0.83529412]],\n",
       "\n",
       "        [[-0.83529412, -0.74901961, -0.78039216],\n",
       "         [-0.7254902 , -0.58431373, -0.70196078],\n",
       "         [-0.74117647, -0.61568627, -0.73333333],\n",
       "         ...,\n",
       "         [-0.78823529, -0.67843137, -0.74117647],\n",
       "         [-0.71764706, -0.56862745, -0.6627451 ],\n",
       "         [-0.77254902, -0.54509804, -0.71764706]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.01960784,  0.01176471,  0.01176471],\n",
       "         [-0.01176471, -0.01960784, -0.01176471],\n",
       "         [-0.06666667, -0.08235294, -0.10588235],\n",
       "         ...,\n",
       "         [-0.64705882, -0.37254902, -0.76470588],\n",
       "         [-0.61568627, -0.38039216, -0.7254902 ],\n",
       "         [-0.46666667, -0.23921569, -0.54509804]],\n",
       "\n",
       "        [[-0.01176471, -0.03529412, -0.01960784],\n",
       "         [-0.06666667, -0.0745098 , -0.06666667],\n",
       "         [-0.09019608, -0.09803922, -0.09019608],\n",
       "         ...,\n",
       "         [-0.70980392, -0.4745098 , -0.79607843],\n",
       "         [-0.76470588, -0.58431373, -0.80392157],\n",
       "         [-0.6627451 , -0.49803922, -0.74901961]],\n",
       "\n",
       "        [[ 0.04313725,  0.05098039,  0.04313725],\n",
       "         [ 0.01176471,  0.02745098,  0.00392157],\n",
       "         [ 0.02745098,  0.00392157,  0.01960784],\n",
       "         ...,\n",
       "         [-0.71764706, -0.52941176, -0.75686275],\n",
       "         [-0.74117647, -0.59215686, -0.78039216],\n",
       "         [-0.78039216, -0.64705882, -0.80392157]]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = DataLoader('horse2zebra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定網路參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = InstanceNormalization()(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = InstanceNormalization()(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    # Image input\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d4, d3, gf*4)\n",
    "    u2 = deconv2d(u1, d2, gf*2)\n",
    "    u3 = deconv2d(u2, d1, gf)\n",
    "\n",
    "    u4 = UpSampling2D(size=2)(u3)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "    return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立鑑別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if normalization:\n",
    "            d = InstanceNormalization()(d)\n",
    "        return d\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    d1 = d_layer(img, df, normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Translate images to opposite domain\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / translated = Fake)\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # Total disciminator loss\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                    [valid, valid,\n",
    "                                                    imgs_A, imgs_B,\n",
    "                                                    imgs_A, imgs_B])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                    % ( epoch, epochs,\n",
    "                                                                        batch_i, data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        np.mean(g_loss[1:3]),\n",
    "                                                                        np.mean(g_loss[3:5]),\n",
    "                                                                        np.mean(g_loss[5:6]),\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(epoch, batch_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 圖片取樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, batch_i):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3\n",
    "\n",
    "    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "    print(img_A)\n",
    "    # Demo (for GIF)\n",
    "    #imgs_A = data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "    #imgs_B = data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "    # Translate images to the other domain\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # Translate back to original domain\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "dataset_name = 'horse2zebra'\n",
    "data_loader = DataLoader(dataset_name=dataset_name,\n",
    "                              img_res=(img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 32\n",
    "df = 64\n",
    "\n",
    "# Loss weights\n",
    "lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "lambda_id = 0.1 * lambda_cycle    # Identity loss\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminators\n",
    "d_A = build_discriminator()\n",
    "d_B = build_discriminator()\n",
    "d_A.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "d_B.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------\n",
    "\n",
    "# Build the generators\n",
    "g_AB = build_generator()\n",
    "g_BA = build_generator()\n",
    "\n",
    "# Input images from both domains\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# Translate images to the other domain\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)\n",
    "# Translate images back to original domain\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)\n",
    "# Identity mapping of images\n",
    "img_A_id = g_BA(img_A)\n",
    "img_B_id = g_AB(img_B)\n",
    "\n",
    "# For the combined model we will only train the generators\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)\n",
    "\n",
    "# Combined model trains generators to fool discriminators\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                reconstr_A, reconstr_B,\n",
    "                                img_A_id, img_B_id ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                       'mae', 'mae',\n",
    "                       'mae', 'mae'],\n",
    "                    loss_weights=[  1, 1,\n",
    "                                    lambda_cycle, lambda_cycle,\n",
    "                                    lambda_id, lambda_id ],\n",
    "                    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:480: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:483: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/1067] [D loss: 15.753410, acc:   8%] [G loss: 115.228897, adv: 49.493828, recon: 0.741623, id: 0.754162] time: 0:00:23.732913 \n",
      "Tensor(\"input_41:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 0/200] [Batch 1/1067] [D loss: 23.113737, acc:   8%] [G loss: 26.469727, adv: 6.824684, recon: 0.582142, id: 0.612913] time: 0:00:25.231449 \n",
      "[Epoch 0/200] [Batch 2/1067] [D loss: 22.822628, acc:   7%] [G loss: 88.451233, adv: 36.514187, recon: 0.701521, id: 0.665340] time: 0:00:26.538433 \n",
      "[Epoch 0/200] [Batch 3/1067] [D loss: 37.407707, acc:   3%] [G loss: 47.848083, adv: 17.128796, recon: 0.618246, id: 0.535509] time: 0:00:27.824406 \n",
      "[Epoch 0/200] [Batch 4/1067] [D loss: 25.085812, acc:   4%] [G loss: 20.478729, adv: 3.069910, recon: 0.642286, id: 0.856659] time: 0:00:29.128828 \n",
      "[Epoch 0/200] [Batch 5/1067] [D loss: 7.215264, acc:  19%] [G loss: 18.576794, adv: 1.953830, recon: 0.661761, id: 0.720773] time: 0:00:30.402171 \n",
      "[Epoch 0/200] [Batch 6/1067] [D loss: 2.527235, acc:  33%] [G loss: 17.596693, adv: 2.330682, recon: 0.577903, id: 0.634551] time: 0:00:31.677719 \n",
      "[Epoch 0/200] [Batch 7/1067] [D loss: 1.505281, acc:  32%] [G loss: 15.154392, adv: 1.502932, recon: 0.541559, id: 0.672269] time: 0:00:32.916404 \n",
      "[Epoch 0/200] [Batch 8/1067] [D loss: 1.002444, acc:  31%] [G loss: 14.792823, adv: 1.142934, recon: 0.567318, id: 0.540940] time: 0:00:34.159839 \n",
      "[Epoch 0/200] [Batch 9/1067] [D loss: 0.913632, acc:  33%] [G loss: 14.836917, adv: 1.180788, recon: 0.570184, id: 0.500219] time: 0:00:35.405860 \n",
      "[Epoch 0/200] [Batch 10/1067] [D loss: 0.998140, acc:  43%] [G loss: 15.755928, adv: 1.336188, recon: 0.598783, id: 0.503017] time: 0:00:36.690058 \n",
      "[Epoch 0/200] [Batch 11/1067] [D loss: 0.488435, acc:  51%] [G loss: 14.423240, adv: 1.032528, recon: 0.560971, id: 0.488355] time: 0:00:38.037787 \n",
      "[Epoch 0/200] [Batch 12/1067] [D loss: 0.623311, acc:  45%] [G loss: 14.050111, adv: 0.784783, recon: 0.569678, id: 0.510967] time: 0:00:39.383268 \n",
      "[Epoch 0/200] [Batch 13/1067] [D loss: 0.590297, acc:  42%] [G loss: 13.025885, adv: 1.257871, recon: 0.482521, id: 0.417664] time: 0:00:40.712562 \n",
      "[Epoch 0/200] [Batch 14/1067] [D loss: 0.623510, acc:  47%] [G loss: 14.654087, adv: 0.741609, recon: 0.602784, id: 0.570572] time: 0:00:42.055864 \n",
      "[Epoch 0/200] [Batch 15/1067] [D loss: 0.814114, acc:  33%] [G loss: 12.235188, adv: 1.044460, recon: 0.458657, id: 0.431421] time: 0:00:43.300805 \n",
      "[Epoch 0/200] [Batch 16/1067] [D loss: 1.403916, acc:  27%] [G loss: 21.061090, adv: 4.454193, recon: 0.553749, id: 0.585603] time: 0:00:44.611091 \n",
      "[Epoch 0/200] [Batch 17/1067] [D loss: 1.233051, acc:  27%] [G loss: 12.966634, adv: 1.129966, recon: 0.492373, id: 0.386487] time: 0:00:45.922743 \n",
      "[Epoch 0/200] [Batch 18/1067] [D loss: 0.686743, acc:  41%] [G loss: 13.374534, adv: 0.622039, recon: 0.549360, id: 0.549839] time: 0:00:47.281959 \n",
      "[Epoch 0/200] [Batch 19/1067] [D loss: 0.446049, acc:  52%] [G loss: 12.145031, adv: 0.953148, recon: 0.465448, id: 0.507945] time: 0:00:48.598836 \n",
      "[Epoch 0/200] [Batch 20/1067] [D loss: 1.575526, acc:  32%] [G loss: 12.862449, adv: 1.018110, recon: 0.493580, id: 0.424748] time: 0:00:49.933153 \n",
      "[Epoch 0/200] [Batch 21/1067] [D loss: 0.366202, acc:  59%] [G loss: 10.380903, adv: 0.716457, recon: 0.406316, id: 0.367555] time: 0:00:51.275264 \n",
      "[Epoch 0/200] [Batch 22/1067] [D loss: 0.356104, acc:  53%] [G loss: 12.222063, adv: 0.668918, recon: 0.496170, id: 0.471077] time: 0:00:52.651061 \n",
      "[Epoch 0/200] [Batch 23/1067] [D loss: 0.477690, acc:  48%] [G loss: 11.607428, adv: 0.673880, recon: 0.464734, id: 0.406183] time: 0:00:54.009499 \n",
      "[Epoch 0/200] [Batch 24/1067] [D loss: 0.503170, acc:  51%] [G loss: 13.236558, adv: 1.502509, recon: 0.469439, id: 0.404064] time: 0:00:55.413668 \n",
      "[Epoch 0/200] [Batch 25/1067] [D loss: 0.423764, acc:  55%] [G loss: 12.494672, adv: 0.985232, recon: 0.477700, id: 0.450245] time: 0:00:56.788678 \n",
      "[Epoch 0/200] [Batch 26/1067] [D loss: 1.160106, acc:  23%] [G loss: 10.199408, adv: 0.518177, recon: 0.415979, id: 0.443230] time: 0:00:58.193415 \n",
      "[Epoch 0/200] [Batch 27/1067] [D loss: 0.829649, acc:  23%] [G loss: 9.315309, adv: 0.257185, recon: 0.400993, id: 0.317185] time: 0:00:59.689365 \n",
      "[Epoch 0/200] [Batch 28/1067] [D loss: 0.586048, acc:  40%] [G loss: 11.097491, adv: 0.820420, recon: 0.434078, id: 0.351287] time: 0:01:01.157978 \n",
      "[Epoch 0/200] [Batch 29/1067] [D loss: 0.387571, acc:  51%] [G loss: 10.717334, adv: 0.911329, recon: 0.400897, id: 0.448791] time: 0:01:02.664617 \n",
      "[Epoch 0/200] [Batch 30/1067] [D loss: 0.319849, acc:  60%] [G loss: 9.159995, adv: 0.582744, recon: 0.366375, id: 0.223661] time: 0:01:04.155303 \n",
      "[Epoch 0/200] [Batch 31/1067] [D loss: 0.702551, acc:  33%] [G loss: 9.637746, adv: 0.674088, recon: 0.377679, id: 0.410512] time: 0:01:05.528739 \n",
      "[Epoch 0/200] [Batch 32/1067] [D loss: 0.828244, acc:  25%] [G loss: 11.871968, adv: 1.591952, recon: 0.400779, id: 0.331334] time: 0:01:06.974572 \n",
      "[Epoch 0/200] [Batch 33/1067] [D loss: 1.065089, acc:  25%] [G loss: 8.272708, adv: 0.449153, recon: 0.335708, id: 0.312754] time: 0:01:08.365841 \n",
      "[Epoch 0/200] [Batch 34/1067] [D loss: 0.627610, acc:  25%] [G loss: 8.373202, adv: 0.588896, recon: 0.326383, id: 0.295748] time: 0:01:09.767732 \n",
      "[Epoch 0/200] [Batch 35/1067] [D loss: 0.399704, acc:  42%] [G loss: 10.773890, adv: 0.959116, recon: 0.408158, id: 0.330539] time: 0:01:11.132439 \n",
      "[Epoch 0/200] [Batch 36/1067] [D loss: 0.353498, acc:  54%] [G loss: 9.213998, adv: 0.628233, recon: 0.359771, id: 0.299385] time: 0:01:12.542607 \n",
      "[Epoch 0/200] [Batch 37/1067] [D loss: 0.599971, acc:  33%] [G loss: 8.751705, adv: 0.506171, recon: 0.354043, id: 0.277971] time: 0:01:13.993311 \n",
      "[Epoch 0/200] [Batch 38/1067] [D loss: 0.385426, acc:  50%] [G loss: 10.514462, adv: 0.863033, recon: 0.404059, id: 0.318344] time: 0:01:15.416186 \n",
      "[Epoch 0/200] [Batch 39/1067] [D loss: 0.679453, acc:  37%] [G loss: 8.595201, adv: 0.487447, recon: 0.350626, id: 0.296715] time: 0:01:16.815288 \n",
      "[Epoch 0/200] [Batch 40/1067] [D loss: 0.646271, acc:  24%] [G loss: 9.700619, adv: 0.823092, recon: 0.361878, id: 0.322468] time: 0:01:18.298892 \n",
      "[Epoch 0/200] [Batch 41/1067] [D loss: 0.760551, acc:  40%] [G loss: 8.198180, adv: 0.918537, recon: 0.286910, id: 0.323482] time: 0:01:19.733158 \n",
      "[Epoch 0/200] [Batch 42/1067] [D loss: 0.475269, acc:  41%] [G loss: 9.588577, adv: 0.630851, recon: 0.379833, id: 0.362874] time: 0:01:21.356495 \n",
      "[Epoch 0/200] [Batch 43/1067] [D loss: 1.433452, acc:  28%] [G loss: 10.069094, adv: 0.545519, recon: 0.413487, id: 0.326233] time: 0:01:23.003283 \n",
      "[Epoch 0/200] [Batch 44/1067] [D loss: 0.383122, acc:  55%] [G loss: 10.004582, adv: 0.903593, recon: 0.370039, id: 0.396825] time: 0:01:24.570826 \n",
      "[Epoch 0/200] [Batch 45/1067] [D loss: 0.342978, acc:  58%] [G loss: 8.279205, adv: 0.541916, recon: 0.327968, id: 0.183973] time: 0:01:26.126306 \n",
      "[Epoch 0/200] [Batch 46/1067] [D loss: 0.969408, acc:  30%] [G loss: 8.888992, adv: 0.781403, recon: 0.332878, id: 0.354059] time: 0:01:27.674944 \n",
      "[Epoch 0/200] [Batch 47/1067] [D loss: 0.661001, acc:  39%] [G loss: 8.271119, adv: 0.649200, recon: 0.315194, id: 0.284712] time: 0:01:29.352573 \n",
      "[Epoch 0/200] [Batch 48/1067] [D loss: 0.267543, acc:  60%] [G loss: 7.784609, adv: 0.677637, recon: 0.293619, id: 0.184969] time: 0:01:30.987148 \n",
      "[Epoch 0/200] [Batch 49/1067] [D loss: 0.750884, acc:  43%] [G loss: 11.017821, adv: 0.861498, recon: 0.425205, id: 0.383290] time: 0:01:32.500287 \n",
      "[Epoch 0/200] [Batch 50/1067] [D loss: 1.209450, acc:  19%] [G loss: 9.341371, adv: 0.641625, recon: 0.368026, id: 0.434593] time: 0:01:33.955161 \n",
      "[Epoch 0/200] [Batch 51/1067] [D loss: 0.403334, acc:  58%] [G loss: 11.070149, adv: 0.999759, recon: 0.415767, id: 0.440343] time: 0:01:35.609638 \n",
      "[Epoch 0/200] [Batch 52/1067] [D loss: 0.958372, acc:  19%] [G loss: 9.063406, adv: 1.036882, recon: 0.320797, id: 0.262568] time: 0:01:37.191833 \n",
      "[Epoch 0/200] [Batch 53/1067] [D loss: 0.367384, acc:  51%] [G loss: 9.617123, adv: 1.312810, recon: 0.321305, id: 0.224581] time: 0:01:38.769538 \n",
      "[Epoch 0/200] [Batch 54/1067] [D loss: 0.971139, acc:  30%] [G loss: 10.677417, adv: 0.790073, recon: 0.410797, id: 0.443560] time: 0:01:40.407182 \n",
      "[Epoch 0/200] [Batch 55/1067] [D loss: 1.035792, acc:  38%] [G loss: 8.796988, adv: 1.174660, recon: 0.294319, id: 0.292438] time: 0:01:41.982003 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 56/1067] [D loss: 1.066366, acc:  34%] [G loss: 7.848130, adv: 0.416051, recon: 0.317220, id: 0.285440] time: 0:01:43.533464 \n",
      "[Epoch 0/200] [Batch 57/1067] [D loss: 0.511955, acc:  55%] [G loss: 7.943526, adv: 1.114923, recon: 0.257166, id: 0.304260] time: 0:01:45.090834 \n",
      "[Epoch 0/200] [Batch 58/1067] [D loss: 0.745723, acc:  30%] [G loss: 7.115406, adv: 0.396686, recon: 0.287907, id: 0.298413] time: 0:01:46.685741 \n",
      "[Epoch 0/200] [Batch 59/1067] [D loss: 0.301646, acc:  56%] [G loss: 6.539166, adv: 0.907699, recon: 0.213888, id: 0.198518] time: 0:01:48.288851 \n",
      "[Epoch 0/200] [Batch 60/1067] [D loss: 0.433684, acc:  40%] [G loss: 6.698278, adv: 0.473452, recon: 0.261832, id: 0.229230] time: 0:01:49.829604 \n",
      "[Epoch 0/200] [Batch 61/1067] [D loss: 0.189022, acc:  73%] [G loss: 8.186588, adv: 0.861599, recon: 0.294414, id: 0.201906] time: 0:01:51.442059 \n",
      "[Epoch 0/200] [Batch 62/1067] [D loss: 0.215521, acc:  69%] [G loss: 7.111234, adv: 0.653087, recon: 0.266139, id: 0.224688] time: 0:01:52.997950 \n",
      "[Epoch 0/200] [Batch 63/1067] [D loss: 0.573386, acc:  22%] [G loss: 6.302784, adv: 0.412022, recon: 0.250205, id: 0.232790] time: 0:01:54.621236 \n",
      "[Epoch 0/200] [Batch 64/1067] [D loss: 0.285794, acc:  65%] [G loss: 10.244183, adv: 1.163787, recon: 0.361379, id: 0.384375] time: 0:01:56.317036 \n",
      "[Epoch 0/200] [Batch 65/1067] [D loss: 0.517038, acc:  30%] [G loss: 6.406983, adv: 0.428981, recon: 0.253911, id: 0.226526] time: 0:01:58.300383 \n",
      "[Epoch 0/200] [Batch 66/1067] [D loss: 0.271135, acc:  60%] [G loss: 6.952080, adv: 0.627245, recon: 0.259584, id: 0.305659] time: 0:02:00.076121 \n",
      "[Epoch 0/200] [Batch 67/1067] [D loss: 0.760825, acc:  14%] [G loss: 9.069839, adv: 0.812365, recon: 0.339828, id: 0.287775] time: 0:02:01.694495 \n",
      "[Epoch 0/200] [Batch 68/1067] [D loss: 0.549294, acc:  44%] [G loss: 7.666474, adv: 0.839661, recon: 0.270065, id: 0.367423] time: 0:02:03.326878 \n",
      "[Epoch 0/200] [Batch 69/1067] [D loss: 0.582819, acc:  32%] [G loss: 7.719639, adv: 0.445199, recon: 0.311852, id: 0.297956] time: 0:02:04.878959 \n",
      "[Epoch 0/200] [Batch 70/1067] [D loss: 0.470137, acc:  45%] [G loss: 8.543125, adv: 0.615973, recon: 0.335394, id: 0.278958] time: 0:02:06.490017 \n",
      "[Epoch 0/200] [Batch 71/1067] [D loss: 0.675251, acc:  27%] [G loss: 9.128579, adv: 0.535899, recon: 0.366117, id: 0.380719] time: 0:02:08.071364 \n",
      "[Epoch 0/200] [Batch 72/1067] [D loss: 0.359663, acc:  58%] [G loss: 10.241480, adv: 0.920861, recon: 0.385367, id: 0.447073] time: 0:02:09.715105 \n",
      "[Epoch 0/200] [Batch 73/1067] [D loss: 0.212118, acc:  76%] [G loss: 7.937383, adv: 0.684833, recon: 0.296396, id: 0.321220] time: 0:02:11.354602 \n",
      "[Epoch 0/200] [Batch 74/1067] [D loss: 0.134274, acc:  84%] [G loss: 9.712080, adv: 0.984890, recon: 0.353322, id: 0.334872] time: 0:02:12.940602 \n",
      "[Epoch 0/200] [Batch 75/1067] [D loss: 1.501513, acc:  15%] [G loss: 9.571164, adv: 1.227170, recon: 0.323867, id: 0.335287] time: 0:02:14.545232 \n",
      "[Epoch 0/200] [Batch 76/1067] [D loss: 0.873984, acc:  16%] [G loss: 7.176764, adv: 0.497764, recon: 0.279705, id: 0.284067] time: 0:02:16.068391 \n",
      "[Epoch 0/200] [Batch 77/1067] [D loss: 0.442933, acc:  45%] [G loss: 8.319677, adv: 0.684238, recon: 0.316546, id: 0.350895] time: 0:02:17.629243 \n",
      "[Epoch 0/200] [Batch 78/1067] [D loss: 0.402065, acc:  51%] [G loss: 7.198891, adv: 0.688697, recon: 0.264828, id: 0.196215] time: 0:02:19.220899 \n",
      "[Epoch 0/200] [Batch 79/1067] [D loss: 0.719200, acc:  31%] [G loss: 5.984844, adv: 0.529275, recon: 0.223268, id: 0.263060] time: 0:02:20.805896 \n",
      "[Epoch 0/200] [Batch 80/1067] [D loss: 0.370883, acc:  51%] [G loss: 7.053963, adv: 0.596494, recon: 0.265923, id: 0.252894] time: 0:02:22.434272 \n",
      "[Epoch 0/200] [Batch 81/1067] [D loss: 0.761490, acc:  21%] [G loss: 6.747080, adv: 0.736145, recon: 0.243445, id: 0.239532] time: 0:02:24.021393 \n",
      "[Epoch 0/200] [Batch 82/1067] [D loss: 0.936054, acc:  22%] [G loss: 6.929433, adv: 0.915219, recon: 0.232637, id: 0.245028] time: 0:02:25.678793 \n",
      "[Epoch 0/200] [Batch 83/1067] [D loss: 0.868192, acc:  23%] [G loss: 8.916435, adv: 0.599255, recon: 0.347195, id: 0.402510] time: 0:02:27.297407 \n",
      "[Epoch 0/200] [Batch 84/1067] [D loss: 0.675925, acc:  53%] [G loss: 7.778273, adv: 1.334133, recon: 0.232600, id: 0.170100] time: 0:02:28.859009 \n",
      "[Epoch 0/200] [Batch 85/1067] [D loss: 1.049086, acc:  21%] [G loss: 6.430270, adv: 0.637332, recon: 0.233339, id: 0.212564] time: 0:02:30.482146 \n",
      "[Epoch 0/200] [Batch 86/1067] [D loss: 1.029515, acc:  55%] [G loss: 8.052073, adv: 1.060886, recon: 0.270440, id: 0.318927] time: 0:02:32.148875 \n",
      "[Epoch 0/200] [Batch 87/1067] [D loss: 0.679137, acc:  35%] [G loss: 8.312996, adv: 1.117660, recon: 0.275782, id: 0.273358] time: 0:02:33.882518 \n",
      "[Epoch 0/200] [Batch 88/1067] [D loss: 0.427712, acc:  41%] [G loss: 6.857974, adv: 0.746340, recon: 0.246462, id: 0.156835] time: 0:02:35.509485 \n",
      "[Epoch 0/200] [Batch 89/1067] [D loss: 0.533625, acc:  35%] [G loss: 6.834117, adv: 0.554114, recon: 0.261516, id: 0.235530] time: 0:02:37.137083 \n",
      "[Epoch 0/200] [Batch 90/1067] [D loss: 0.453024, acc:  27%] [G loss: 7.326891, adv: 0.754476, recon: 0.263980, id: 0.279098] time: 0:02:38.771262 \n",
      "[Epoch 0/200] [Batch 91/1067] [D loss: 0.622196, acc:  23%] [G loss: 9.292106, adv: 0.403392, recon: 0.392846, id: 0.324962] time: 0:02:40.386296 \n",
      "[Epoch 0/200] [Batch 92/1067] [D loss: 0.358235, acc:  37%] [G loss: 6.254412, adv: 0.469815, recon: 0.243072, id: 0.188577] time: 0:02:42.043883 \n",
      "[Epoch 0/200] [Batch 93/1067] [D loss: 0.488199, acc:  31%] [G loss: 5.958939, adv: 0.358711, recon: 0.238904, id: 0.243572] time: 0:02:43.783535 \n",
      "[Epoch 0/200] [Batch 94/1067] [D loss: 0.269722, acc:  52%] [G loss: 6.553674, adv: 0.550217, recon: 0.248425, id: 0.204580] time: 0:02:45.450562 \n",
      "[Epoch 0/200] [Batch 95/1067] [D loss: 0.585090, acc:  13%] [G loss: 10.387616, adv: 0.386570, recon: 0.437017, id: 0.533822] time: 0:02:47.077782 \n",
      "[Epoch 0/200] [Batch 96/1067] [D loss: 0.594301, acc:  23%] [G loss: 8.472361, adv: 0.810402, recon: 0.313369, id: 0.245987] time: 0:02:48.726901 \n",
      "[Epoch 0/200] [Batch 97/1067] [D loss: 0.569407, acc:  26%] [G loss: 5.151970, adv: 0.368637, recon: 0.201265, id: 0.183216] time: 0:02:50.548985 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e5f9a25c1b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-62943769253e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                     [valid, valid,\n\u001b[1;32m     40\u001b[0m                                                     \u001b[0mimgs_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                                     imgs_A, imgs_B])\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs=200, batch_size=1, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
